
#
# Example of configuration for running a Focused Crawl
#

# Store pages classified as irrelevant pages by the target page classifier

# target_storage.data_format.type: FILES
# target_storage.data_format.files.max_file_size: 134217728

target_storage.store_negative_pages: true
#target_storage.data_format.type: FILESYSTEM_HTML
target_storage.english_language_detection_enabled: true
# target_storage.data_format.type: WARC                    # enable WARC file format
# target_storage.data_format.warc.compress: true           # enable GZIP compression
# target_storage.data_format.warc.max_file_size: 262144000 # maximum file size in 

target_storage.data_format.type: ELASTICSEARCH
target_storage.data_format.elasticsearch.rest.hosts:
  - http://localhost:9200



# Limit the max number of pages crawled per domain, in order to avoid crawling
# too many pages from same somain and favor discovery o new domains
link_storage.max_pages_per_domain: 10000

# Disable "seed scope" to allow crawl pages from any domain
link_storage.link_strategy.use_scope: false

# Set initial link classifier a simple one
link_storage.link_classifier.type: LinkClassifierBaseline

# Train a new link classifier while the crawler is running. This allows
# the crawler automatically learn how to prioritize links in order to
# efficiently locate relevant content while avoiding the retrieval of
# irrelevant content.
link_storage.online_learning.enabled: true
link_storage.online_learning.type: FORWARD_CLASSIFIER_BINARY
link_storage.online_learning.learning_limit: 1000

# Allways select top-k links with highest priority to be scheduled
link_storage.link_selector: TopkLinkSelector

# Configure the minimum time interval (in milliseconds) to wait between requests
# to the same host to avoid overloading servers. If you are crawling your own
# web site, you can descrease this value to speed-up the crawl.
link_storage.scheduler.host_min_access_interval: 5000

# Configure the User-Agent of the crawler
crawler_manager.downloader.user_agent.name: Macintosh
crawler_manager.downloader.user_agent.url: "Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0"

crawler_manager.downloader.download_thread_pool_size: 100
crawler_manager.downloader.max_retry_count: 2
crawler_manager.downloader.valid_mime_types:
 - text/xml
 - text/html
 - text/plain
 - application/x-asp
 - application/xhtml+xml
 - application/vnd.wap.xhtml+xml
